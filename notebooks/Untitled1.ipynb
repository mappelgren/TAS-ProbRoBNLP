{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b5c75c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from itertools import product\n",
    "\n",
    "TraceCell = namedtuple('TraceCell', ['T', 'text', 'p'])\n",
    "\n",
    "\n",
    "class PCFG:\n",
    "    \n",
    "    def __init__(self, production_rules: list = None, start_symbol: NonTerminal = None, probability_of_rules: dict = None):\n",
    "        \n",
    "#         self.non_terminals = non_terminals if non_terminals is not None else []\n",
    "#         self.terminals = terminals if terminals is not None else []\n",
    "        \n",
    "        rules, start_symbol = to_cnf([production_rules, start_symbol])\n",
    "        print(rules, start_symbol)\n",
    "        self.lexicon = defaultdict(list)\n",
    "        self.rules = []\n",
    "        for rule in rules:\n",
    "            if isinstance(rule.RHS[0], Terminal):\n",
    "                self.lexicon[rule.RHS[0].name].append((rule.LHS, rule.p))\n",
    "            else:\n",
    "                self.rules.append(rule)\n",
    "\n",
    "        self.production_rules = rules\n",
    "        self.start_symbol = start_symbol\n",
    "        self.probability_of_rules = probability_of_rules if probability_of_rules is not None else {}\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        \n",
    "        words = sentence.split()\n",
    "        \n",
    "        trace = defaultdict(list)\n",
    "        n = len(words) + 1\n",
    "        \n",
    "        for i in range(1, n):\n",
    "            word = words[i-1]\n",
    "            trace[(i-1, i)] = [TraceCell(term, word, p) for (term, p) in self.lexicon[word]]\n",
    "            \n",
    "        print(trace)\n",
    "        for j in range(2, n):\n",
    "            for i in range(j-1, -1, -1):\n",
    "                for k in range(i+1, j):\n",
    "                    for c1, c2 in product(trace[(i, k)], trace[(k, j)]):\n",
    "                        \n",
    "                        for c3, p in self.use_rules(c1.T, c2.T):\n",
    "                            trace[(i, j)].append(TraceCell(c3, f\"{c1[1]} {c2[1]}\", p * c1.p * c2.p))\n",
    "                            \n",
    "        return trace[(0, n-1)], trace\n",
    "    \n",
    "    def use_rules(self, c1, c2):\n",
    "        for rule in self.rules:\n",
    "            left, right = rule.RHS\n",
    "            parent = rule.LHS\n",
    "\n",
    "            if left == c1 and right == c2:\n",
    "                yield parent, rule.p #f\"{sem[app_order[0]]}({sem[app_order[1]]})\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9f4a1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DT <- a (0.1), NN <- cup (0.5), DT <- the (0.7), IN <- on (1), NN <- table (0.5), NR0 <- put (1), S <- NR0 NPP (1.0), NR1 <- NP IN (1), NPP <- NR1 NP (1.0), NP <- DT NN (1.0)] S\n",
      "defaultdict(<class 'list'>, {(0, 1): [TraceCell(T=NR0, text='put', p=1)], (1, 2): [TraceCell(T=DT, text='a', p=0.1)], (2, 3): [TraceCell(T=NN, text='cup', p=0.5)], (3, 4): [TraceCell(T=IN, text='on', p=1)], (4, 5): [TraceCell(T=DT, text='the', p=0.7)], (5, 6): [TraceCell(T=NN, text='table', p=0.5)]})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([TraceCell(T=S, text='put a cup on the table', p=0.017499999999999998)],\n",
       " defaultdict(list,\n",
       "             {(0, 1): [TraceCell(T=NR0, text='put', p=1)],\n",
       "              (1, 2): [TraceCell(T=DT, text='a', p=0.1)],\n",
       "              (2, 3): [TraceCell(T=NN, text='cup', p=0.5)],\n",
       "              (3, 4): [TraceCell(T=IN, text='on', p=1)],\n",
       "              (4, 5): [TraceCell(T=DT, text='the', p=0.7)],\n",
       "              (5, 6): [TraceCell(T=NN, text='table', p=0.5)],\n",
       "              (1, 3): [TraceCell(T=NP, text='a cup', p=0.05)],\n",
       "              (0, 2): [],\n",
       "              (2, 4): [],\n",
       "              (1, 4): [TraceCell(T=NR1, text='a cup on', p=0.05)],\n",
       "              (0, 3): [],\n",
       "              (3, 5): [],\n",
       "              (2, 5): [],\n",
       "              (1, 5): [],\n",
       "              (0, 4): [],\n",
       "              (4, 6): [TraceCell(T=NP, text='the table', p=0.35)],\n",
       "              (3, 6): [],\n",
       "              (2, 6): [],\n",
       "              (1,\n",
       "               6): [TraceCell(T=NPP, text='a cup on the table', p=0.017499999999999998)],\n",
       "              (0,\n",
       "               6): [TraceCell(T=S, text='put a cup on the table', p=0.017499999999999998)],\n",
       "              (0, 5): []}))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rules2 = [Rule(S, [A, S, B]),\n",
    "#             Rule(A, [a, A, S]), \n",
    "#             Rule(A, [a]), \n",
    "#             Rule(A, []), \n",
    "#             Rule(B, [S, B, S]),\n",
    "#             Rule(B, [A]),\n",
    "#             Rule(B, [b, b])]\n",
    "\n",
    "# rules3 = [Rule(S, [A, a]),\n",
    "#           Rule(S, [B]),\n",
    "#           Rule(A, [b]),\n",
    "#           Rule(A, [B]),\n",
    "#           Rule(B, [A]),\n",
    "#           Rule(B, [a])]\n",
    "# g3 = [rules3, S]\n",
    "\n",
    "# pcfg2 = PCFG(rules3, S)\n",
    "\n",
    "# pcfg1 = PCFG(rules2, S)\n",
    "# pcfg1.parse(\"a a a b b b b\")\n",
    "\n",
    "pcfg3 = PCFG(rules1, S)\n",
    "out = pcfg3.parse(\"put a cup on the table\")\n",
    "# pcfg2.parse(\"b a a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8a253b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S0 <- S, S <- A S B, A <- a A S, A <- a, A <- , B <- S B S, B <- A, B <- b b]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bd892302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar2 start\n",
      "S <- A S B (1)\n",
      "A <- a A S (0.3) | a (0.2) |  (0.5)\n",
      "B <- S B S (0.1) | A (0.6) | b b (0.3)\n",
      "grammar2 end\n",
      "NR0 <- a (1)\n",
      "A <- NR0 S (0.3) | NR2 S (0.3) | a (0.4)\n",
      "NR2 <- NR0 A (1)\n",
      "B <- S S (0.04285714285714286) | NR3 S (0.09999999999999999) | NR1 NR1 (0.4285714285714286) | NR0 S (0.1285714285714286) | NR6 S (0.1285714285714286) | a (0.17142857142857146)\n",
      "NR3 <- S B (1)\n",
      "NR1 <- b (1)\n",
      "S <- S B (0.4117647058823529) | A S (0.17647058823529413) | NR4 B (0.4117647058823529)\n",
      "NR4 <- A S (1)\n",
      "S0 <- S B (0.4117647058823529) | A S (0.17647058823529413) | NR5 B (0.4117647058823529)\n",
      "NR5 <- A S (1)\n",
      "NR6 <- NR0 A (1)\n",
      "grammar3 start\n",
      "S <- A a (0.6) | B (0.4)\n",
      "A <- b (0.7) | B (0.3)\n",
      "B <- A (0.2) | a (0.8)\n",
      "grammar3 end\n",
      "NR0 <- a (1)\n",
      "S <- A NR0 (0.6) | a (0.32000000000000006) | b (0.05957446808510639) | a (0.02042553191489362)\n",
      "A <- b (0.7446808510638298) | a (0.2553191489361702)\n",
      "grammar1 start\n",
      "VB <- put (1.0)\n",
      "DT <- a (0.1) | the (0.7)\n",
      "NN <- cup (0.5) | table (0.5)\n",
      "IN <- on (1)\n",
      "S <- put NPP (1.0)\n",
      "NPP <- NP IN NP (1.0)\n",
      "NP <- DT NN (1.0)\n",
      "grammar1 end\n",
      "DT <- a (0.1) | the (0.7)\n",
      "NN <- cup (0.5) | table (0.5)\n",
      "IN <- on (1)\n",
      "NR0 <- put (1)\n",
      "S <- NR0 NPP (1.0)\n",
      "NR1 <- NP IN (1)\n",
      "NPP <- NR1 NP (1.0)\n",
      "NP <- DT NN (1.0)\n",
      "grammar4 start\n",
      "S <- B A (1)\n",
      "A <- a (0.7) |  (0.3)\n",
      "B <- b (1.0)\n",
      "grammar4 end\n",
      "S <- B A (0.7) | b (0.3)\n",
      "B <- b (1.0)\n",
      "A <- a (1.0)\n",
      "grammar5 start\n",
      "S <- a a (0.7) | b (0.3)\n",
      "grammar5 end\n",
      "NR0 <- a (1)\n",
      "S <- NR0 NR0 (0.7) | b (0.3)\n",
      "grammar5 start\n",
      "S <- A B B (0.7) | B B (0.3)\n",
      "B <- b (1)\n",
      "A <- a (1)\n",
      "grammar5 end\n",
      "NR0 <- A B (1)\n",
      "S <- NR0 B (0.7) | B B (0.3)\n",
      "B <- b (1)\n",
      "A <- a (1)\n"
     ]
    }
   ],
   "source": [
    "class NonTerminal():\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "        \n",
    "class Terminal():\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "     \n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "        \n",
    "        \n",
    "class Rule():\n",
    "    \n",
    "    def __init__(self, LHS, RHS, p=1):\n",
    "        self.LHS = LHS\n",
    "        self.RHS = RHS\n",
    "        self.p = p\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.LHS} <- {' '.join([str(t) for t in self.RHS])} ({self.p})\"\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.LHS == other.LHS and all([r1 == r2 for r1, r2 in zip(self.RHS, other.RHS)]) \n",
    "                and len(self.RHS) == len(other.RHS) and self.p == other.p\n",
    "               )\n",
    "        \n",
    "put = Terminal('put')\n",
    "a = Terminal('a')\n",
    "cup = Terminal('cup')\n",
    "on = Terminal('on')\n",
    "the = Terminal('the')\n",
    "table = Terminal('table')\n",
    "\n",
    "VB = NonTerminal('VB')\n",
    "DT = NonTerminal('DT')\n",
    "IN = NonTerminal('IN')\n",
    "NN = NonTerminal('NN')\n",
    "NP = NonTerminal('NP')\n",
    "S = NonTerminal('S')\n",
    "NPP = NonTerminal(\"NPP\")\n",
    "PP = NonTerminal('PP')\n",
    "\n",
    "rules1 = [Rule(VB, [put], 1.0), \n",
    "           Rule(DT, [a], 0.1),\n",
    "           Rule(NN, [cup], 0.5),\n",
    "           Rule(DT, [the], 0.7),\n",
    "           Rule(IN, [on], 1),\n",
    "           Rule(NN, [table], 0.5),\n",
    "           Rule(S, [put, NPP], 1.0),\n",
    "           Rule(NPP, [NP, IN, NP], 1.0),\n",
    "           Rule(NP, [DT, NN], 1.0)]\n",
    "\n",
    "\n",
    "b = Terminal('b')\n",
    "A = NonTerminal('A')\n",
    "B = NonTerminal('B')\n",
    "\n",
    "rules2 = [Rule(S, [A, S, B], 1),\n",
    "            Rule(A, [a, A, S], 0.3), \n",
    "            Rule(A, [a], 0.2), \n",
    "            Rule(A, [], 0.5), \n",
    "            Rule(B, [S, B, S], 0.1),\n",
    "            Rule(B, [A], 0.6),\n",
    "            Rule(B, [b, b], 0.3)]\n",
    "\n",
    "\n",
    "g1 = [rules1, S]\n",
    "g2 = [rules2, S]\n",
    "\n",
    "\n",
    "def normalise_rule(grammar, lhs):\n",
    "    rules, s = grammar\n",
    "    \n",
    "    to_normalise = [r for r in rules if r.LHS == lhs]\n",
    "    p_norm = sum([r.p for r in to_normalise])\n",
    "    if p_norm == 1:\n",
    "        return grammar\n",
    "    else:\n",
    "        new_rules = [Rule(r.LHS, r.RHS, r.p/p_norm) for r in to_normalise]\n",
    "        \n",
    "    rules = [r for r in rules if r not in to_normalise] + new_rules\n",
    "    return (rules, s)\n",
    "        \n",
    "\n",
    "def to_cnf(grammar):\n",
    "    \n",
    "    added_rule_nr = 0\n",
    "    \n",
    "    rule_variable_name = 'NR'\n",
    "    \n",
    "    rules, s = grammar\n",
    "    start = s\n",
    "    \n",
    "    for rule in rules:\n",
    "        lhs = rule.LHS\n",
    "        rhs = rule.RHS\n",
    "        if start in rhs:\n",
    "            s0 = NonTerminal(start.name+\"0\")\n",
    "            rules.insert(0, Rule(s0, [start]))\n",
    "            start = s0\n",
    "\n",
    "    change = True\n",
    "\n",
    "    while change:\n",
    "        change = False\n",
    "        for rule in rules:\n",
    "            lhs = rule.LHS\n",
    "            rhs = rule.RHS\n",
    "            if rhs == []:\n",
    "#                 if lhs == start:\n",
    "#                     continue\n",
    "                change = True\n",
    "                new_rules = []\n",
    "                for rule2 in rules:\n",
    "                    if rule != rule2:\n",
    "                        if lhs in rule2.RHS:\n",
    "                            p = rule.p * rule2.p\n",
    "                            p2 = (1-rule.p) * rule2.p\n",
    "                            new_rules.append(Rule(rule2.LHS, [t for t in rule2.RHS if t != lhs], p))\n",
    "                            rule2 = Rule(rule2.LHS, rule2.RHS, p2)\n",
    "                        new_rules.append(rule2)\n",
    "                        \n",
    "                rules, s = normalise_rule((new_rules, s), lhs)\n",
    "                \n",
    "            if len(rhs) == 1 and lhs == rhs[0]:\n",
    "                change = True\n",
    "                new_rules = [r for r in rules if r != rule]\n",
    "                rules = new_rules\n",
    "                rules, s = normalise_rule((rules, s), rule.LHS)\n",
    "                \n",
    "    change = True\n",
    "    \n",
    "    guf = []\n",
    "    \n",
    "    \n",
    "    while change:\n",
    "        change = False\n",
    "        to_remove = []\n",
    "        for rule in rules:\n",
    "            lhs = rule.LHS\n",
    "            rhs = rule.RHS\n",
    "            \n",
    "            \n",
    "            if len(rhs) == 1 and isinstance(rhs[0], NonTerminal):\n",
    "                \n",
    "                to_remove.append(rule)\n",
    "            else:\n",
    "                guf.append(rule)\n",
    "                \n",
    "        for rule in to_remove:\n",
    "            change = True\n",
    "            lhs = rule.LHS\n",
    "            rhs = rule.RHS[0]\n",
    "\n",
    "            \n",
    "            rhsides = [(r.RHS, r.p) for r in rules if r.LHS == rhs]\n",
    "            \n",
    "            for (r,p) in rhsides:\n",
    "                guf.append(Rule(lhs, r, rule.p * p))\n",
    "        \n",
    "        removals = [r.LHS for r in guf if r.RHS[0] == r.LHS and len(r.RHS) == 1]\n",
    "        guf = [r for r in guf if not(r.RHS[0] == r.LHS and len(r.RHS) == 1)]\n",
    "        \n",
    "        for l in removals:\n",
    "            guf, s = normalise_rule((guf, s), l)\n",
    "        \n",
    "        rules = guf\n",
    "        guf = []\n",
    "    \n",
    "    rules, start = remove_non_reachable_state([rules, start])\n",
    "        \n",
    "    new_terminal_rules = {}\n",
    "    \n",
    "    new_rules = []\n",
    "    for rule in rules:\n",
    "        lhs = rule.LHS\n",
    "        rhs = rule.RHS\n",
    "        \n",
    "        if len(rhs) > 1 and any([isinstance(t, Terminal) for t in rhs]):\n",
    "            new_rhs = []\n",
    "            for t in rhs:\n",
    "                if isinstance(t, Terminal):\n",
    "                    if t not in new_terminal_rules:\n",
    "                        l = NonTerminal(rule_variable_name + str(added_rule_nr))\n",
    "                        added_rule_nr += 1\n",
    "                        r = Rule(l, [t], 1)\n",
    "                        new_rules.append(r)\n",
    "                        new_terminal_rules[t] = l\n",
    "                    \n",
    "                    new_rhs.append(new_terminal_rules[t])\n",
    "                else:\n",
    "                    new_rhs.append(t)\n",
    "            new_rules.append(Rule(lhs, new_rhs, rule.p))\n",
    "        else:\n",
    "            new_rules.append(rule)\n",
    "    \n",
    "    rules = new_rules\n",
    "    new_rules = []             \n",
    "    change = True\n",
    "    while change:\n",
    "        change = False\n",
    "        for rule in rules:\n",
    "            if len(rule.RHS) > 2:\n",
    "                change = True\n",
    "                \n",
    "                l = NonTerminal(rule_variable_name + str(added_rule_nr))\n",
    "                added_rule_nr += 1\n",
    "                \n",
    "                lhs = rule.LHS\n",
    "                rhs1 = rule.RHS[:2]\n",
    "                rhs2 = [l] + rule.RHS[2:]\n",
    "                \n",
    "                new_rules.append(Rule(l, rhs1, 1))\n",
    "                new_rules.append(Rule(lhs, rhs2, rule.p))\n",
    "                \n",
    "            else:\n",
    "                new_rules.append(rule)\n",
    "    \n",
    "        rules = new_rules\n",
    "        new_rules = []\n",
    "            \n",
    "    return [rules, start]\n",
    "\n",
    "\n",
    "\n",
    "print(\"grammar2 start\")\n",
    "display_grammar(g2)\n",
    "print(\"grammar2 end\")\n",
    "display_grammar(to_cnf(g2))\n",
    "\n",
    "\n",
    "\n",
    "rules3 = [Rule(S, [A, a], 0.6),\n",
    "          Rule(S, [B], 0.4),\n",
    "          Rule(A, [b], 0.7),\n",
    "          Rule(A, [B], 0.3),\n",
    "          Rule(B, [A], 0.2),\n",
    "          Rule(B, [a], 0.8)]\n",
    "g3 = [rules3, S]\n",
    "\n",
    "print(\"grammar3 start\")\n",
    "display_grammar(g3)\n",
    "g = to_cnf(g3)\n",
    "print(\"grammar3 end\")\n",
    "display_grammar(g)\n",
    "\n",
    "print(\"grammar1 start\")\n",
    "display_grammar(g1)\n",
    "print(\"grammar1 end\")\n",
    "display_grammar(to_cnf(g1))\n",
    "\n",
    "\n",
    "rules4 = [Rule(S, [B, A], 1),\n",
    "          Rule(A, [a], 0.7),\n",
    "          Rule(A, [], 0.3),\n",
    "          Rule(B, [b], 1.0)]\n",
    "g4 = [rules4, S]\n",
    "print(\"grammar4 start\")\n",
    "display_grammar(g4)\n",
    "print(\"grammar4 end\")\n",
    "display_grammar(to_cnf(g4))\n",
    "\n",
    "rules5 = [Rule(S, [a, a], 0.7),\n",
    "          Rule(S, [b], 0.3),]\n",
    "g5 = [rules5, S]\n",
    "print(\"grammar5 start\")\n",
    "display_grammar(g5)\n",
    "print(\"grammar5 end\")\n",
    "display_grammar(to_cnf(g5))\n",
    "\n",
    "rules5 = [Rule(S, [A, B, B], 0.7),\n",
    "          Rule(S, [B, B], 0.3),\n",
    "         Rule(B, [b], 1),\n",
    "         Rule(A, [a], 1)]\n",
    "g5 = [rules5, S]\n",
    "print(\"grammar5 start\")\n",
    "display_grammar(g5)\n",
    "print(\"grammar5 end\")\n",
    "display_grammar(to_cnf(g5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "181d28ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR0 <- a (1)\n",
      "S <- A NR0 (1) | a (1) | b (1) | a (1)\n",
      "A <- b (0.7) | a (1)\n"
     ]
    }
   ],
   "source": [
    "def display_grammar(g):\n",
    "    rules, s = g\n",
    "    \n",
    "    grammar = defaultdict(list)\n",
    "    \n",
    "    for rule in rules:\n",
    "        grammar[rule.LHS].append((rule.RHS, rule.p))\n",
    "        \n",
    "    for key, value in grammar.items():\n",
    "        print(f\"{key} <- {' | '.join([' '.join([str(a) for a in t]) + f' ({p})' for t, p in value])}\")\n",
    "\n",
    "display_grammar(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7283ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75f7becf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([S<-Aa, A<-b, S<-a, A<-a, S<-b, S<-a], S)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reachable_states(grammar):\n",
    "    rules, s = grammar\n",
    "    \n",
    "    reached_states = []\n",
    "    \n",
    "    search(rules, s, reached_states)\n",
    "    \n",
    "    return reached_states\n",
    "    \n",
    "def search(rules, s, reached_states):\n",
    "    \n",
    "    reached_states.append(s)\n",
    "    \n",
    "    relevant_rules = [r for r in rules if r.LHS == s]\n",
    "    \n",
    "    for r in relevant_rules:\n",
    "        nodes = next_nodes(r, reached_states)\n",
    "        \n",
    "        for node in nodes:\n",
    "            search(rules, node, reached_states)\n",
    "        \n",
    "\n",
    "\n",
    "def next_nodes(rule, reached_states):\n",
    "    return [t for t in rule.RHS if isinstance(t, NonTerminal) and t not in reached_states]\n",
    "        \n",
    "def remove_non_reachable_state(g):\n",
    "    rs = reachable_states(g)\n",
    "    \n",
    "    rules, s = g\n",
    "    rules = [r for r in rules if r.LHS in rs]\n",
    "    \n",
    "    return (rules, s)\n",
    "\n",
    "remove_non_reachable_state(g)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746eeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eda52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('put', 'VB'),\n",
    " ('a', 'DT'),\n",
    " ('cup', 'NN'),\n",
    " ('on', 'IN'),\n",
    " ('the', 'DT'),\n",
    " ('table', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e37802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'a mug should be on it',\n",
       " 'ents': [],\n",
       " 'sents': [{'start': 0, 'end': 21}],\n",
       " 'tokens': [{'id': 0,\n",
       "   'start': 0,\n",
       "   'end': 1,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DT',\n",
       "   'dep': 'det',\n",
       "   'head': 1},\n",
       "  {'id': 1,\n",
       "   'start': 2,\n",
       "   'end': 5,\n",
       "   'pos': 'PROPN',\n",
       "   'tag': 'NNP',\n",
       "   'dep': 'nsubj',\n",
       "   'head': 3},\n",
       "  {'id': 2,\n",
       "   'start': 6,\n",
       "   'end': 12,\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'MD',\n",
       "   'dep': 'aux',\n",
       "   'head': 3},\n",
       "  {'id': 3,\n",
       "   'start': 13,\n",
       "   'end': 15,\n",
       "   'pos': 'AUX',\n",
       "   'tag': 'VB',\n",
       "   'dep': 'ROOT',\n",
       "   'head': 3},\n",
       "  {'id': 4,\n",
       "   'start': 16,\n",
       "   'end': 18,\n",
       "   'pos': 'ADP',\n",
       "   'tag': 'IN',\n",
       "   'dep': 'prep',\n",
       "   'head': 3},\n",
       "  {'id': 5,\n",
       "   'start': 19,\n",
       "   'end': 21,\n",
       "   'pos': 'PRON',\n",
       "   'tag': 'PRP',\n",
       "   'dep': 'pobj',\n",
       "   'head': 4}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sents = nlp(u'a mug should be on it')\n",
    "\n",
    "sents.to_json()\n",
    "\n",
    "# def get_words_and_tags(sentence):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     sents = nlp(sentence)\n",
    "    \n",
    "#     for token in sents['tokens']:\n",
    "#         word = sents[token['start']:token['end']]\n",
    "#         tag = token['tag']\n",
    "        \n",
    "#         yield (word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7013265c-2ba0-41bb-85da-c05066998182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'a green mug should be on the tray',\n",
       " 'ents': [],\n",
       " 'sents': [{'start': 0, 'end': 33}],\n",
       " 'tokens': [{'id': 0,\n",
       "   'start': 0,\n",
       "   'end': 1,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DT',\n",
       "   'dep': 'det',\n",
       "   'head': 2},\n",
       "  {'id': 1,\n",
       "   'start': 2,\n",
       "   'end': 7,\n",
       "   'pos': 'ADJ',\n",
       "   'tag': 'JJ',\n",
       "   'dep': 'amod',\n",
       "   'head': 2},\n",
       "  {'id': 2,\n",
       "   'start': 8,\n",
       "   'end': 11,\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NN',\n",
       "   'dep': 'nsubj',\n",
       "   'head': 4},\n",
       "  {'id': 3,\n",
       "   'start': 12,\n",
       "   'end': 18,\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'MD',\n",
       "   'dep': 'aux',\n",
       "   'head': 4},\n",
       "  {'id': 4,\n",
       "   'start': 19,\n",
       "   'end': 21,\n",
       "   'pos': 'AUX',\n",
       "   'tag': 'VB',\n",
       "   'dep': 'ROOT',\n",
       "   'head': 4},\n",
       "  {'id': 5,\n",
       "   'start': 22,\n",
       "   'end': 24,\n",
       "   'pos': 'ADP',\n",
       "   'tag': 'IN',\n",
       "   'dep': 'prep',\n",
       "   'head': 4},\n",
       "  {'id': 6,\n",
       "   'start': 25,\n",
       "   'end': 28,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DT',\n",
       "   'dep': 'det',\n",
       "   'head': 7},\n",
       "  {'id': 7,\n",
       "   'start': 29,\n",
       "   'end': 33,\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NN',\n",
       "   'dep': 'pobj',\n",
       "   'head': 5}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nlp(u'a green mug should be on the tray')\n",
    "\n",
    "sents.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef460e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_and_tags(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    sents = nlp(sentence).to_json()\n",
    "    \n",
    "    # print(sents['tokens'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for token in sents['tokens']:\n",
    "        word = sentence[token['start']:token['end']]\n",
    "        tag = token['tag']\n",
    "        dep = token['dep']\n",
    "        pos = token['pos']\n",
    "        head = token['head']\n",
    "        yield {'word':word, 'tag':tag, 'dep':dep, 'pos':pos, 'head':head}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf7ffe7-efaa-4a83-a3d3-b014d8c101b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"An Acura Integra is parked in the lot\",\n",
    "             \"There is an Acura Integra parked in the lot.\",\n",
    "             \"John parked an Acura Integra in the lot.\", \n",
    "             \"John gave his Acura Integra a bath\",\n",
    "             \"Inside his Acura Integra, John showed Susan his new CD player\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533485ed-e3af-48db-8a73-bd1f7f463034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'An', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 2}\n",
      "{'word': 'Acura', 'tag': 'NNP', 'dep': 'compound', 'pos': 'PROPN', 'head': 2}\n",
      "{'word': 'Integra', 'tag': 'NNP', 'dep': 'nsubjpass', 'pos': 'PROPN', 'head': 4}\n",
      "{'word': 'is', 'tag': 'VBZ', 'dep': 'auxpass', 'pos': 'AUX', 'head': 4}\n",
      "{'word': 'parked', 'tag': 'VBN', 'dep': 'ROOT', 'pos': 'VERB', 'head': 4}\n",
      "{'word': 'in', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP', 'head': 4}\n",
      "{'word': 'the', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 7}\n",
      "{'word': 'lot', 'tag': 'NN', 'dep': 'pobj', 'pos': 'NOUN', 'head': 5}\n",
      "\n",
      "{'word': 'There', 'tag': 'EX', 'dep': 'expl', 'pos': 'PRON', 'head': 1}\n",
      "{'word': 'is', 'tag': 'VBZ', 'dep': 'ROOT', 'pos': 'AUX', 'head': 1}\n",
      "{'word': 'an', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 4}\n",
      "{'word': 'Acura', 'tag': 'NNP', 'dep': 'compound', 'pos': 'PROPN', 'head': 4}\n",
      "{'word': 'Integra', 'tag': 'NNPS', 'dep': 'attr', 'pos': 'PROPN', 'head': 1}\n",
      "{'word': 'parked', 'tag': 'VBN', 'dep': 'acl', 'pos': 'VERB', 'head': 4}\n",
      "{'word': 'in', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP', 'head': 5}\n",
      "{'word': 'the', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 8}\n",
      "{'word': 'lot', 'tag': 'NN', 'dep': 'pobj', 'pos': 'NOUN', 'head': 6}\n",
      "{'word': '.', 'tag': '.', 'dep': 'punct', 'pos': 'PUNCT', 'head': 1}\n",
      "\n",
      "{'word': 'John', 'tag': 'NNP', 'dep': 'nsubj', 'pos': 'PROPN', 'head': 1}\n",
      "{'word': 'parked', 'tag': 'VBD', 'dep': 'ROOT', 'pos': 'VERB', 'head': 1}\n",
      "{'word': 'an', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 4}\n",
      "{'word': 'Acura', 'tag': 'NNP', 'dep': 'compound', 'pos': 'PROPN', 'head': 4}\n",
      "{'word': 'Integra', 'tag': 'NNP', 'dep': 'dobj', 'pos': 'PROPN', 'head': 1}\n",
      "{'word': 'in', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP', 'head': 1}\n",
      "{'word': 'the', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 7}\n",
      "{'word': 'lot', 'tag': 'NN', 'dep': 'pobj', 'pos': 'NOUN', 'head': 5}\n",
      "{'word': '.', 'tag': '.', 'dep': 'punct', 'pos': 'PUNCT', 'head': 1}\n",
      "\n",
      "{'word': 'John', 'tag': 'NNP', 'dep': 'nsubj', 'pos': 'PROPN', 'head': 1}\n",
      "{'word': 'gave', 'tag': 'VBD', 'dep': 'ROOT', 'pos': 'VERB', 'head': 1}\n",
      "{'word': 'his', 'tag': 'PRP$', 'dep': 'poss', 'pos': 'DET', 'head': 4}\n",
      "{'word': 'Acura', 'tag': 'NNP', 'dep': 'compound', 'pos': 'PROPN', 'head': 4}\n",
      "{'word': 'Integra', 'tag': 'NNP', 'dep': 'dative', 'pos': 'PROPN', 'head': 1}\n",
      "{'word': 'a', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 6}\n",
      "{'word': 'bath', 'tag': 'NN', 'dep': 'dobj', 'pos': 'NOUN', 'head': 1}\n",
      "\n",
      "{'word': 'Inside', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP', 'head': 6}\n",
      "{'word': 'his', 'tag': 'PRP$', 'dep': 'poss', 'pos': 'DET', 'head': 3}\n",
      "{'word': 'Acura', 'tag': 'NNP', 'dep': 'compound', 'pos': 'PROPN', 'head': 3}\n",
      "{'word': 'Integra', 'tag': 'NNP', 'dep': 'pobj', 'pos': 'PROPN', 'head': 0}\n",
      "{'word': ',', 'tag': ',', 'dep': 'punct', 'pos': 'PUNCT', 'head': 6}\n",
      "{'word': 'John', 'tag': 'NNP', 'dep': 'nsubj', 'pos': 'PROPN', 'head': 6}\n",
      "{'word': 'showed', 'tag': 'VBD', 'dep': 'ROOT', 'pos': 'VERB', 'head': 6}\n",
      "{'word': 'Susan', 'tag': 'NNP', 'dep': 'dative', 'pos': 'PROPN', 'head': 6}\n",
      "{'word': 'his', 'tag': 'PRP$', 'dep': 'poss', 'pos': 'DET', 'head': 11}\n",
      "{'word': 'new', 'tag': 'JJ', 'dep': 'amod', 'pos': 'ADJ', 'head': 11}\n",
      "{'word': 'CD', 'tag': 'NN', 'dep': 'compound', 'pos': 'NOUN', 'head': 11}\n",
      "{'word': 'player', 'tag': 'NN', 'dep': 'dobj', 'pos': 'NOUN', 'head': 6}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    for line in get_words_and_tags(sentence):\n",
    "        print(line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65a27b0c-6cb5-4c27-94de-8522a082c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_referents(sentence):\n",
    "    for word in sentence:\n",
    "        if 'obj' in word['dep'] or 'subj' in word['dep']:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c3aaf-15e4-420b-9f59-fa0b7766b146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d457f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'start': 0, 'end': 1, 'pos': 'DET', 'tag': 'DT', 'dep': 'det', 'head': 1}, {'id': 1, 'start': 2, 'end': 5, 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'head': 3}, {'id': 2, 'start': 6, 'end': 12, 'pos': 'VERB', 'tag': 'MD', 'dep': 'aux', 'head': 3}, {'id': 3, 'start': 13, 'end': 15, 'pos': 'AUX', 'tag': 'VB', 'dep': 'ROOT', 'head': 3}, {'id': 4, 'start': 16, 'end': 18, 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'head': 3}, {'id': 5, 'start': 19, 'end': 21, 'pos': 'PRON', 'tag': 'PRP', 'dep': 'pobj', 'head': 4}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'a', 'tag': 'DT', 'dep': 'det', 'pos': 'DET', 'head': 1},\n",
       " {'word': 'mug', 'tag': 'NNP', 'dep': 'nsubj', 'pos': 'PROPN', 'head': 3},\n",
       " {'word': 'should', 'tag': 'MD', 'dep': 'aux', 'pos': 'VERB', 'head': 3},\n",
       " {'word': 'be', 'tag': 'VB', 'dep': 'ROOT', 'pos': 'AUX', 'head': 3},\n",
       " {'word': 'on', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP', 'head': 3},\n",
       " {'word': 'it', 'tag': 'PRP', 'dep': 'pobj', 'pos': 'PRON', 'head': 4}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_words_and_tags('a mug should be on it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f010d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'start': 0, 'end': 3, 'pos': 'VERB', 'tag': 'VB', 'dep': 'ROOT', 'head': 0}, {'id': 1, 'start': 4, 'end': 5, 'pos': 'DET', 'tag': 'DT', 'dep': 'det', 'head': 2}, {'id': 2, 'start': 6, 'end': 9, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'head': 0}, {'id': 3, 'start': 10, 'end': 12, 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'head': 0}, {'id': 4, 'start': 13, 'end': 16, 'pos': 'DET', 'tag': 'DT', 'dep': 'det', 'head': 5}, {'id': 5, 'start': 17, 'end': 22, 'pos': 'NOUN', 'tag': 'NN', 'dep': 'pobj', 'head': 3}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'put', 'tag': 'VB', 'dep': 'ROOT', 'pos': 'VERB'},\n",
       " {'word': 'a', 'tag': 'DT', 'dep': 'det', 'pos': 'DET'},\n",
       " {'word': 'cup', 'tag': 'NN', 'dep': 'dobj', 'pos': 'NOUN'},\n",
       " {'word': 'on', 'tag': 'IN', 'dep': 'prep', 'pos': 'ADP'},\n",
       " {'word': 'the', 'tag': 'DT', 'dep': 'det', 'pos': 'DET'},\n",
       " {'word': 'table', 'tag': 'NN', 'dep': 'pobj', 'pos': 'NOUN'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_words_and_tags('put a cup on the table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e064a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grammar = PCFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse(self, sentence):\n",
    "        words = sentence.split()\n",
    "        \n",
    "        trace = defaultdict(list)\n",
    "        n = len(words) + 1\n",
    "        \n",
    "        for i in range(1, n):\n",
    "            word = words[i-1]\n",
    "            trace[(i-1, i)] = [[(syntax, semantics), word] for syntax, semantics, in self.lexicon[word]]\n",
    "        \n",
    "        for j in range(2, n):\n",
    "            for i in range(j-1, -1, -1):\n",
    "                for k in range(i+1, j):\n",
    "                    for c1, c2 in product(trace[(i, k)], trace[(k, j)]):\n",
    "                        \n",
    "                        for c3 in self.use_rules(c1[0], c2[0]):\n",
    "                            trace[(i, j)].append([c3, f\"{c1[1]} {c2[1]}\"])\n",
    "                            \n",
    "        return trace[(0, n-1)]\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
